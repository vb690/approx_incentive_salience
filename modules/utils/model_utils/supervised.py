import time

import numpy as np

from tensorflow.keras.layers import Dense, Embedding, TimeDistributed
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Activation, LeakyReLU, ReLU, ELU
from tensorflow.keras.layers import Dropout, SpatialDropout1D
from tensorflow.keras.utils import plot_model

from kerastuner import HyperModel

from tensorflow.keras.models import Model


class _AbstractHyperEstimator(HyperModel):
    """
    Protected Class storing common methods inherited by actual estimator
    """
    ACTIVATIONS = {
        'relu': ReLU,
        'elu': ELU,
        'lelu': LeakyReLU
    }

    def __getstate__(self):
        state = dict(self.__dict__)
        del state['_model']
        return state

    def _generate_embedding_block(self, hp, input_tensor, input_dim,  tag,
                                  max_dim=224):
        """"Protected method generating an embedding block, composed by an
        embedding and flatten layer.

        Args:
            - input_tensor:a keras tensor, input of the EB
            - input_dim: an integer, number of unique categories
            - output_dim: an integer, number of values composing the dense
                          rappresentation of each category
            - input_length: an integer, lenght of the sequcne of categorical
                            features
            - tag: a string, identifier for the block

        Returns:
            - embedding: is a keras tensor, portion of graph generated by EB
        """
        embedding = Embedding(
            input_dim=input_dim,
            output_dim=hp.Int(
                min_value=8,
                max_value=max_dim,
                step=8,
                name='embedding_{}_output'.format(tag)
            ),
            input_length=None,
            name='embedding_layer_{}'.format(tag)
            )(input_tensor)

        embedding = SpatialDropout1D(
            self.dropout_rate,
            name='sp_dropout_{}'.format(tag)
        )(embedding)

        return embedding

    def _generate_fully_connected_block(self, hp, input_tensor, tag, prob,
                                        max_layers=5, max_dim=224):
        """Protected method genearting a block of fuly connected layers (FCB)
        Each block can have an arbitrary number of layer each one having an
        arbitrary number of units.

        Args:
            - input_tensor: a keras tensor, input of the FCB
            - layers:       a list, specifying number of layers and units

              example: [100, 100, 100] <-- 3 layers 100 units each

            - activation:   a string, activation function applied to each unit
            - dropout:      a float, probability of a unit being dropped
            - tag:          a string, identifier for the block
            - prob:         a bolean, wheather to apply dropout at test time
                            allowing the model to be probabilistic

        Returns:
            - fully_connected: a keras tesnor, portion of graph generated
                               by FCB
        """
        layers = hp.Int(
            min_value=1,
            max_value=max_layers,
            name='dense_layers_{}'.format(tag)
        )
        for layer in range(layers):

            if layer == 0:
                fully_connected = Dense(
                    units=hp.Int(
                        min_value=8,
                        max_value=max_dim,
                        step=8,
                        name='dense_units_{}_{}'.format(layer, tag)
                    ),
                    name='{}_dense_layer_{}'.format(layer, tag)
                    )(input_tensor)
            else:
                fully_connected = Dense(
                    units=hp.Int(
                        min_value=8,
                        max_value=max_dim,
                        step=8,
                        name='dense_units_{}_{}'.format(layer, tag)
                    ),
                    name='{}_dense_layer_{}'.format(layer, tag)
                    )(fully_connected)

            chos_act = hp.Choice(
                            values=['elu', 'relu', 'lelu'],
                            name='dense_activation_{}_{}'.format(layer, tag),
                        )
            fully_connected = Activation(
                self.ACTIVATIONS[chos_act](),
                name='{}_{}_activation_dense_layer_{}'.format(
                    layer, chos_act, tag)
                )(fully_connected)

            if self.dropout_spatial:
                fully_connected = SpatialDropout1D(
                    self.dropout_rate,
                    name='sp_dropout_{}_{}'.format(layer, tag)
                )(fully_connected)
            else:
                fully_connected = Dropout(
                    self.dropout_rate,
                    name='dropout_{}_{}'.format(layer, tag)
                )(fully_connected)

        return fully_connected

    def _generate_recurrent_block(self, hp, input_tensor, tag, max_layers=2,
                                  max_dim=224):
        """
        """
        layers = hp.Int(
            min_value=1,
            max_value=max_layers,
            name='lstm_layers_{}'.format(tag)
        )
        for layer in range(layers):

            if layer == 0:
                recurrent = LSTM(
                    units=hp.Int(
                        min_value=8,
                        max_value=max_dim,
                        step=8,
                        name='lstm_units_{}_{}'.format(layer, tag)
                    ),
                    return_sequences=True,
                    name='{}_lstm_layer_{}'.format(layer, tag)
                )(input_tensor)
            else:
                recurrent = LSTM(
                    units=hp.Int(
                        min_value=8,
                        max_value=max_dim,
                        step=8,
                        name='lstm_units_{}_{}'.format(layer, tag)
                    ),
                    return_sequences=True,
                    name='{}_lstm_layer_{}'.format(layer, tag)
                )(recurrent)

            recurrent = SpatialDropout1D(
                self.dropout_rate,
                name='{}_sp_dropout_{}'.format(layer, tag)
            )(recurrent)

        return recurrent

    def _generate_hier_recurrent_block(self, hp, input_tensor, tag,
                                       max_layers=2):
        """
        """
        first_hier_layers = hp.Int(
            min_value=1,
            max_value=max_layers,
            name='first_hier_lstm_layers_{}'.format(tag)
        )
        second_hier_layers = hp.Int(
            min_value=1,
            max_value=max_layers,
            name='second_hier_lstm_layers_{}'.format(tag)
        )

        # first level of hierarchy
        for layer in range(first_hier_layers):

            if layer == 0:
                first_hier_recurrent = TimeDistributed(
                    LSTM(
                        units=hp.Int(
                            min_value=5,
                            max_value=200,
                            step=5,
                            name=f'first_hier_lstm_units_{layer}_{tag}'
                            ),
                        return_sequences=first_hier_layers > 1,
                        name=f'first_hier_{layer}_lstm_layer_{tag}'
                    )
                )(input_tensor)
            else:
                first_hier_recurrent = LSTM(
                    units=hp.Int(
                        min_value=5,
                        max_value=200,
                        step=5,
                        name=f'first_hier_lstm_units_{layer}_{tag}'
                    ),
                    name=f'first_hier_{layer}_lstm_layer_{tag}'
                )(first_hier_recurrent)

        # second level of hierarchy
        for layer in range(second_hier_layers):

            if layer == 0:
                second_hier_recurrent = TimeDistributed(
                    LSTM(
                        units=hp.Int(
                            min_value=5,
                            max_value=200,
                            step=5,
                            name=f'second_hier_lstm_units_{layer}_{tag}'
                            ),
                        return_sequences=True,
                        name=f'second_hier_{layer}_lstm_layer_{tag}'
                    )
                )(first_hier_recurrent)
            else:
                second_hier_recurrent = LSTM(
                    units=hp.Int(
                        min_value=5,
                        max_value=200,
                        step=5,
                        name=f'second_hier_lstm_units_{layer}_{tag}'
                    ),
                    name=f'second_hier_{layer}_lstm_layer_{tag}'
                )(second_hier_recurrent)

        return second_hier_recurrent

    def get_para_count(self):
        """
        Returns:
            -num_parameters: an integer specifying the number of parameters
                             for the model
        """
        num_parameters = self.n_parameters
        return num_parameters

    def get_model(self):
        """
        """
        model = self._model
        return model

    def get_model_tag(self):
        """
        Method for getting the model tag (identifier)

        Returns:
            -model_tag: a string specifying the model tag
        """
        model_tag = self.model_tag
        return model_tag

    def get_fitting_time(self):
        """
        Method for getting the time (on second) taken by the model to fit
        the data (untill convergence)

        Returns:
            -fitting_time: integer specifying the fitting time in seconds
        """
        fitting_time = self.fitting_time
        return fitting_time

    def get_n_epochs(self):
        """
        """
        n_epochs = self.n_epochs
        return n_epochs

    def get_encoder(self, out_layer, inp_layers=None):
        """
        """
        if inp_layers is not None:
            inputs = []
            for inp in self._model.inputs:

                if inp.name in inp_layers:
                    inputs.append(inp)
        else:
            inputs = self._model.inputs

        out = self._model.get_layer(out_layer)
        encoder = Model(inputs, out.output)
        return encoder

    def set_model(self, model):
        """
        """
        setattr(self, '_model', model)
        setattr(self, 'n_parameters', model.count_params())

    def tune(self, tuner, generator, validation_data, epochs=30,
             callbacks=None, verbose=2, **kwargs):
        """
        """
        tuner_obj = tuner(
            hypermodel=self.build,
            **kwargs
        )

        tuner_obj.search(
            generator,
            epochs=epochs,
            validation_data=validation_data,
            callbacks=callbacks,
            verbose=verbose
        )
        tuner_obj.results_summary()
        model = tuner_obj.get_best_models()[0]

        plot_path = 'results\\figures\\architectures'
        plot_model(
            model,
            to_file='{}\\{}.png'.format(plot_path, self.model_tag)
        )
        self.set_model(model)

    def fit(self, **kwargs):
        """
        Method wrapping Keras fit methodd so that is not
        mandatory to access the underlying Keras model

        Args:
         - **kwargs: keyword arguments usually passed to Keras fit method

        """
        start = time.time()
        history = self._model.fit(**kwargs)
        end = time.time()
        setattr(self, 'n_epochs', len(history.history['loss']))
        setattr(self, 'fitting_time', end - start)
        return history

    def predict(self, **kwargs):
        """
        Method wrapping Keras fit method so that is not mandatory to access
        the underlying Keras model

        Args:
          - **kwargs: keyword arguments usually passed to Keras fit method

        Returns:
          - prediction: array or list of arrays
                        storing the model predictions

        """
        prediction = self._model.predict(**kwargs)
        return prediction

    def predict_with_uncertainty(self, X_test, y_test, n_iter, **kwargs):
        """
        Method for perfroming predictions while including uncertainty
        The reference paper is: http://proceedings.mlr.press/v48/gal16.pdf

        In practice, this method will call the predict method of the model
        n_iter times and will 'glue' the reuslts in a numpy array of shape
        (number of targets, number of iteration, shape of the output)

        example with 3 targets, 3 iterations and 3 dimensional output
        (softmax):

        [[[0.6, 0.3, 0.1],
          [0.3, 0.6, 0.1],
          [0.4, 0.4, 0.2]],

         [[0.2, 0.7, 0.1],
          [0.2, 0.6, 0.2],
          [0.1, 0.8, 0.1]],

         [[0.3, 0.5, 0.2],
          [0.1, 0.3, 0.6],
          [0.1, 0.1, 0.8]]]

        For having an estimate of uncertainty we can then compute statistics
        (mu and sigma) over the number of iterations axis or directly visualize
        the distribution of values. It is likely that
        more iterations ---> more time and more precise estimates

        Args:
            - X_test: numpy array, input features for prediction
            - y_test: numpy array, input target from the train set
            - n_iter: integer, number of iterations perfromed
            - batch_size: integer, size of the input batch
            - verbose: integer, ammount of verbosity fro the predict method

        Returns:
            - predictions: numpy array of shape
                           (n_targets, n_iter, target_shape)
                           predictions perfromed by the model
        """
        if self.prob is not True or self.hp_schema['dropout'] == 0:
            raise ValueError('Non-probabilistic is used')
        predictions = np.empty(
            shape=(y_test.shape[0], n_iter, y_test.shape[1])
            )
        for iter in range(n_iter):

            prediction = self._model.predict(
                X_test,
                **kwargs
                )
            predictions[:, iter, :] = prediction

        return predictions
